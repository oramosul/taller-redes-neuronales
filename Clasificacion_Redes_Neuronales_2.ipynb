{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clasificacion_Redes_Neuronales_2.ipynb","provenance":[{"file_id":"https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/02_neural_network_classification_in_tensorflow.ipynb","timestamp":1653674455773}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2WLKnUi6fI6B"},"source":["## Ejemplo de clasificación multiclase\n","\n","Se trabajará con el conjunto de datos llamado [Fashion MNIST built-in](https://github.com/zalandoresearch/fashion-mnist). "]},{"cell_type":"code","metadata":{"id":"zl50sxPTqpw4"},"source":["import tensorflow as tf\n","from tensorflow.keras.datasets import fashion_mnist\n","\n","# Los datos ya están ordenados en entrenamiento y prueba\n","(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8PWdrQsyrBcy"},"source":["# Visualizar la primera instancia de entrenamiento\n","print(f\"Instancia de entrenamiento:\\n{train_data[0]}\\n\") \n","print(f\"Etiqueta de la instancia: {train_labels[0]}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gN5-jr6arj19"},"source":["# Tamaño de los datos\n","train_data.shape, train_labels.shape, test_data.shape, test_labels.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNfIOUUEsJRt"},"source":["# Tamaño de una instancia\n","train_data[0].shape, train_labels[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r2wW0cEfsAve"},"source":["Se tiene 60 000 instancias de entrenamiento de tamaño (28, 28) y una etiqueta, así como 10 000 instancias de prueba de tamaño  (28, 28).\n"]},{"cell_type":"code","metadata":{"id":"RmC2VsWOscKP"},"source":["# Gráfico de una instancia\n","import matplotlib.pyplot as plt\n","\n","plt.imshow(train_data[7]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hzTDEpaYsxga"},"source":["# Etiqueta de la instancia\n","train_labels[7]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZHdVBrCUs10A"},"source":["El nombre de las clases se puede encontrar en el repositorio de Github de [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist#labels)).\n"]},{"cell_type":"code","metadata":{"id":"uGOi32T8s1ai"},"source":["nombres = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n","           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qD40id2tytn"},"source":["# Instancia y su etiqueta\n","plt.imshow(train_data[17], cmap=plt.cm.binary)\n","plt.title(nombres[train_labels[17]]);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rtz9__w3s4JF"},"source":["import random\n","\n","plt.figure(figsize=(7, 7))\n","\n","for i in range(4):\n","  ax = plt.subplot(2, 2, i + 1)\n","  rand_index = random.choice(range(len(train_data)))\n","  plt.imshow(train_data[rand_index], cmap=plt.cm.binary)\n","  plt.title(nombres[train_labels[rand_index]])\n","  plt.axis(False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TLqZif3Rv0Aq"},"source":["Se construirá un modelo que trate sobre la relación entre los valores de los píxeles y sus etiquetas. \n","\n","Dado que es un problema de clasificación multiclase, se necesita realizar algunas modificaciones a la arquitectura:\n","\n","* **Tamaño de entrada**: se tiene que considerar tensores de 28x28 = 784 (alto y ancho de las imágenes), es decir, un vector de tamaño 784\n","\n","* **Tamaño de salida**: tendrá que ser 10, dado que se requiere que el modelo prediga 10 clases diferentes.\n","  * Se modificará la función de activación para que sea de tipo [`\"softmax\"`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax). ESta función brinda una serie de valores entre 0 & 1 (el mismo tamaño que el tamaño de salida, que aproximadamente suma a 1. El índice con el valor más alto es predicho como la clase más probable.\n","* Para la función de costo se requiere utilizar la función de pérdida multiclase. \n","  * Dado que las etiquetas son enteras, se utilizará [`tf.keras.losses.SparseCategoricalCrossentropy()`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy). Si las etiquetas estuviesen en formato one-hot (e.g. algo como `[0, 0, 1, 0, 0...]`), se utilizaría [`tf.keras.losses.CategoricalCrossentropy()`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy).\n","* Se utilizará el parámetro `validation_data` al utilizar la función `fit()`. Esto brindará una idea de cómo se comporta el movimiento en un conjunto de prueba durante el entrenamiento.\n"]},{"cell_type":"code","metadata":{"id":"qUFHuzIpv30K"},"source":["tf.random.set_seed(42)\n","\n","model_1 = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape = (28, 28)), # Capa de entrada (la capa flatten convierte 28x28 en 784)\n","  tf.keras.layers.Dense(4, activation = \"relu\"),\n","  tf.keras.layers.Dense(4, activation = \"relu\"),\n","  tf.keras.layers.Dense(10, activation = \"softmax\") # Capa de salida de tamaño 10 (softmax)\n","])\n","\n","model_1.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), \n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])\n","\n","non_norm_history = model_1.fit(train_data,\n","                               train_labels,\n","                               epochs = 10,\n","                               validation_data = (test_data, test_labels)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hzYWEgoVJ_p"},"source":["# Verificación del modelo\n","model_1.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XRfkre59zSto"},"source":["El modelo brinda aproximadamente 35% de exactitud (accuracy) luego de utilizar 10 épocas. Esto es mejor que algo completamente aleatorio donde, dado que se tiene 10 clases, se predeciría cada una con 10% de probabilidad. \n","\n","Para mejorar este comportamiento se puede normalizar los datos (llevarlos a un rango entre 0 y 1). "]},{"cell_type":"code","metadata":{"id":"tGiweanwz82_"},"source":["# Verificación del mínimo y máximo \n","train_data.min(), train_data.max()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABRKp5U8voV_"},"source":["# Dividir los datos de las imágenes entre el máximo valor (255)\n","train_data = train_data / 255.0\n","test_data = test_data / 255.0\n","\n","# Verificar el valor mínimo y máximo actual\n","train_data.min(), train_data.max()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1QRy7y_K_87"},"source":["tf.random.set_seed(42)\n","\n","model_2 = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(10, activation=\"softmax\")\n","])\n","\n","model_2.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])\n","\n","norm_history = model_2.fit(train_data,\n","                           train_labels,\n","                           epochs = 10,\n","                           validation_data = (test_data, test_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C_I2KNJiMWZ8"},"source":["Para visualizar mejor qué está sucediendo, se puede graficar las curvas de pérdida."]},{"cell_type":"code","metadata":{"id":"zmRcYU7xN1wQ"},"source":["import pandas as pd\n","\n","# Curvas de pérdida de los datos no normalizados\n","pd.DataFrame(non_norm_history.history).plot(title=\"Datos no normalizados\")\n","\n","# Curvas de pérdida de los datos normalizados\n","pd.DataFrame(norm_history.history).plot(title=\"Datos normalizados\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKm8MsACOm4j"},"source":["A partir de estos gráficos, se observa que el modelo con datos normalizados \"aprende\" mucho más rápido que el modelo sin normalización.\n","\n","A continuación se modificará el factor de aprendizaje (learning rate)"]},{"cell_type":"code","metadata":{"id":"LcR_wb4nPSb2"},"source":["tf.random.set_seed(42)\n","\n","model_3 = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(10, activation=\"softmax\")\n","])\n","\n","model_3.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n","                optimizer = tf.keras.optimizers.Adam(),\n","                metrics = [\"accuracy\"])\n","\n","# Creación de un \"callback\" para el factor de aprendizaje\n","lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))\n","\n","# Entrenamiento del modelo\n","find_lr_history = model_3.fit(train_data,\n","                              train_labels,\n","                              epochs = 40,       # tal vez no se requiera 100 épocas\n","                              validation_data=(test_data, test_labels),\n","                              callbacks=[lr_scheduler])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hi2YODTwQ1ie"},"source":["# Gráfico del factor de aprendizaje (learning rate)\n","import numpy as np\n","\n","lrs = 1e-3 * (10**(np.arange(40)/20))\n","plt.semilogx(lrs, find_lr_history.history[\"loss\"]) # eje x-axis en escala logarítmica\n","plt.xlabel(\"Learning rate\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Finding the ideal learning rate\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GqtOjggqWcfS"},"source":["Según el gráfico parece que el valor óptimo podría estar alrededor de 0.001. Se reentrenará el modelo utilizando este factor de aprendizaje"]},{"cell_type":"code","metadata":{"id":"i9KFhAwKXjWd"},"source":["tf.random.set_seed(42)\n","\n","model_4 = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(10, activation=\"softmax\")\n","])\n","\n","model_4.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n","                optimizer = tf.keras.optimizers.Adam(learning_rate=0.001), # Valor \"ideal\"\n","                metrics = [\"accuracy\"])\n","\n","history = model_4.fit(train_data,\n","                      train_labels,\n","                      epochs = 20,\n","                      validation_data = (test_data, test_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ODWXAlnWqri"},"source":["Luego de tener un modelo con un factor de aprendizaje adecuado y con un comportamiento relativamente adecuado, se puede realizar alguna de las siguientes alternativas.\n","\n","* Evaluar su rendimiento utilizando otras métricas de clasificación (como la  [matriz de confusión](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py) o un [reporte de clasificación](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)).\n","* Evaluar sus predicciones (a través de visualizaciónes).\n","* Mejorar la exactitud del modelo (entrenándo por más tiempo o modificando la arquitectura).\n","* Guardar el modelo y exportarlo para uso en una aplicación.\n","\n","Primero se utilizará una matriz de confusión para visualizar las predicciones de las diferentes clases."]},{"cell_type":"code","metadata":{"id":"jK4zA47sYVp5"},"source":["import itertools\n","from sklearn.metrics import confusion_matrix\n","\n","def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15): \n","  \"\"\"Genera una matriz de confusión comparando las predicciones y las etiquetas reales\n","\n","  Si se pasa classes, se etiquetará la matriz de confusión. De lo contrario, se utilizará\n","  valores enteros.\n","\n","  Args:\n","    y_true: Arreglo de etiquetas reales (igual tamaño que y_pred).\n","    y_pred: Arreglo de etiquetas predichas (igual tamaño que y_true).\n","    classes: Arreglo de etiquetas de clase (ejm. en formato de string). Si es `None`, se usa etiquetas enteras\n","    figsize: Tamaño de la salida de la figura (default=(10, 10)).\n","    text_size: Tamaño de la salida del texto  (default=15).\n","  \n","  Returns:\n","    Una matriz de confusión\n","\n","  \"\"\"  \n","  # Creación de la matriz de confusión\n","  cm = confusion_matrix(y_true, y_pred)\n","  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n","  n_classes = cm.shape[0]\n","\n","  # Gráfico de la figura\n","  fig, ax = plt.subplots(figsize=figsize)\n","  cax = ax.matshow(cm, cmap=plt.cm.Blues) \n","  fig.colorbar(cax)\n","\n","  # Hay una lista de clases?\n","  if classes:\n","    labels = classes\n","  else:\n","    labels = np.arange(cm.shape[0])\n","  \n","  # Etiqueta de los ejes\n","  ax.set(title=\"Confusion Matrix\",\n","         xlabel=\"Predicted label\",\n","         ylabel=\"True label\",\n","         xticks=np.arange(n_classes),\n","         yticks=np.arange(n_classes), \n","         xticklabels=labels,\n","         yticklabels=labels)\n","  \n","  # Etiquetas del eje x en la parte inferior\n","  ax.xaxis.set_label_position(\"bottom\")\n","  ax.xaxis.tick_bottom()\n","\n","  # Umbral para los colores\n","  threshold = (cm.max() + cm.min()) / 2.\n","\n","  # Gráfico de texto en cada celda\n","  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","    plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n","             horizontalalignment=\"center\",\n","             color=\"white\" if cm[i, j] > threshold else \"black\",\n","             size=text_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhxrXhrjbjja"},"source":["# Predicciones con el modelo más reciente\n","y_probs = model_4.predict(test_data)\n","\n","# Primeras 5 predicciones\n","y_probs[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eP5zslXbf5ZY"},"source":["La salida es un vector de probabilidades. Para encontrar el valor más alto (la clase más probable) se puede utilizar [`argmax()`](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)."]},{"cell_type":"code","metadata":{"id":"hQvrJbbWf4Es"},"source":["# Clase predicha y su etiqueta, para la primera instancia\n","y_probs[0].argmax(), nombres[y_probs[0].argmax()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozUpeZU6g2An"},"source":["# Convertir todas las predicciones, de probabilidades a etiquetas\n","y_preds = y_probs.argmax(axis=1)\n","\n","# Visualización de las primeras 10 predicciones\n","y_preds[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBMSVSRqcU_m"},"source":["# Matriz de confusión de Scikit learn\n","from sklearn.metrics import confusion_matrix\n","\n","confusion_matrix(y_true = test_labels, \n","                 y_pred = y_preds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLr6daZAbzRi"},"source":["# Matriz de confusión gráfica\n","make_confusion_matrix(y_true = test_labels, \n","                      y_pred = y_preds,\n","                      classes = nombres,\n","                      figsize = (15, 15),\n","                      text_size = 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tH3FhuGEjTu9"},"source":["Parece que el modelo se confunde entre `Shirt` y `T-shirt/top`. Para analizar y tratar de comprender un poco más este problema se puede visualizar algunos ejemplos."]},{"cell_type":"code","metadata":{"id":"2XIuAjgJri9e"},"source":["import random\n","\n","# Función que grafica una imagen aleatoria junto con su predicción\n","def plot_random_image(model, images, true_labels, classes):\n","  # Imagen aleatoria\n","  i = random.randint(0, len(images))\n","  \n","  # Predicciones y etiquetas\n","  target_image = images[i]\n","  pred_probs = model.predict(target_image.reshape(1, 28, 28))\n","  pred_label = classes[pred_probs.argmax()]\n","  true_label = classes[true_labels[i]]\n","\n","  # Gráfico de la imagen\n","  plt.imshow(target_image, cmap=plt.cm.binary)\n","\n","  # Cambiar el color de los títulos según si la predicción es correcta o no\n","  if pred_label == true_label:\n","    color = \"green\"\n","  else:\n","    color = \"red\"\n","  plt.xlabel(\"Pred: {} {:2.0f}% (True: {})\".format(pred_label,\n","                                                   100*tf.reduce_max(pred_probs),\n","                                                   true_label),\n","             color=color)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAAIrpcEumyE"},"source":["# Visualizar una imagen aleatoria y su predicción\n","plot_random_image(model = model_4, \n","                  images = test_data, \n","                  true_labels = test_labels, \n","                  classes = nombres)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pOwM1rqhx6p3"},"source":["### ¿Qué patrones aprende el modelo?\n","\n","Se obtendrá una lista de las capas en el modelo más reciente (`model_14`) usando el atributo `layers`."]},{"cell_type":"code","metadata":{"id":"kcwMsgFuySTi"},"source":["# Capas del modelo más reciente\n","model_4.layers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DXuQmsNX1mGR"},"source":["# Extracción de una capa particular\n","model_4.layers[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W6Cftaib4uG-"},"source":["Se puede encontrar los parámetros aprendidos por cada capa usando`get_weights()`, que retorna los pesos (weights) y sesgos (biases). Cada neurona tiene un vector de sesgo (bias), el cual se encuentra ligado a una matriz de pesos. Los valores de sesgo se inicializan a cero por defecto (pero pueden tener otras inicializaciones) y determinan cuánto los patrones de los pesos correspondientes deben influir en la siguiente capa.\n"]},{"cell_type":"code","metadata":{"id":"WdmZy5xi1srE"},"source":["# Obtener los patrones de una capa en la red\n","weights, biases = model_4.layers[1].get_weights()\n","\n","weights, weights.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndG-h2yz1z2_"},"source":["biases, biases.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_QCUb7GeSGYF"},"source":["# Resumen del modelo\n","model_4.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJD0GqGl3NY0"},"source":["from tensorflow.keras.utils import plot_model\n","\n","# Resumen de las entradas y salidas de cada capa\n","plot_model(model_4, show_shapes=True)"],"execution_count":null,"outputs":[]}]}